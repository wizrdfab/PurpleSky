# Data Setup Guide

This document explains how to set up the large data files that are excluded from git.

## Excluded Directories

The following directories are **not tracked in git** due to their size:

| Directory | Size | Description |
|-----------|------|-------------|
| `data/` | ~19GB | Historical trade data for training |
| `data_collector/` | ~1.2GB | Data collection scripts and cache |
| `models/` | Variable | Trained model artifacts |
| `models_*/` | Variable | Model variants and experiments |
| `logs*/` | Variable | Runtime logs |

## Setting Up Data on a New Machine

### Option 1: Azure Blob Storage (Recommended for Azure VM)

```bash
# Install Azure CLI if not present
# Download: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli

# Login to Azure
az login

# Download data from blob storage
az storage blob download-batch \
    --account-name <storage-account> \
    --source sofia-data \
    --destination ./data

az storage blob download-batch \
    --account-name <storage-account> \
    --source sofia-data-collector \
    --destination ./data_collector
```

### Option 2: Direct Copy (Windows to Windows)

```powershell
# Using robocopy from source machine
robocopy "D:\Fab\omei-eye\Sofia lightgbm - dev\data" "\\AZURE-VM\share\sofia\data" /E /Z /MT:8

robocopy "D:\Fab\omei-eye\Sofia lightgbm - dev\data_collector" "\\AZURE-VM\share\sofia\data_collector" /E /Z /MT:8
```

### Option 3: rsync over SSH (Linux/WSL)

```bash
# From local machine to Azure VM
rsync -avz --progress ./data/ user@azure-vm:/path/to/sofia/data/
rsync -avz --progress ./data_collector/ user@azure-vm:/path/to/sofia/data_collector/
```

### Option 4: SCP (Simple Copy)

```bash
# Compress and copy
tar -czvf data.tar.gz data/
scp data.tar.gz user@azure-vm:/path/to/sofia/

# On remote: extract
ssh user@azure-vm "cd /path/to/sofia && tar -xzvf data.tar.gz"
```

## Creating Empty Directory Structure

To create the required directory structure without data:

```bash
# Run from project root
mkdir -p data data_collector models logs logs_live_trading_funds logs-paper-trading
mkdir -p live_trading_logs live_trading_funds_logs live_results_logs
mkdir -p live_results_funds_logs live_results_funds live_results_simulated
mkdir -p backtests-logs-at-close
```

## Verifying Data Setup

After copying data, verify with:

```bash
# Check data directory size
du -sh data/ data_collector/

# Verify expected files exist
ls data/*.csv | head -5
```

## Notes

- Models can be regenerated by running `train.py`
- Logs are generated during runtime and don't need to be copied
- The `data/` folder is essential for training; without it, only inference is possible
